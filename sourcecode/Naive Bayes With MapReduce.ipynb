{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Naive Bayes With MapReduce.ipynb","provenance":[{"file_id":"1IImduOw_Kkf5_g8yS3BI1nI7m8ASDj0J","timestamp":1593503522717}],"collapsed_sections":["rm-t1R-lNFMW","StqC3ASP8Gen"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"pJyrxJz5NSel"},"source":["# INSTALL SPARK"]},{"cell_type":"code","metadata":{"id":"GEsvZqsINUvA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614598280277,"user_tz":-420,"elapsed":27242,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}},"outputId":"6fc71fd2-7d08-4f68-ae4f-d69ea6c3e2ab"},"source":["# DOWNLOAD DAN INSTALL SPARK, EXTRACT KE CURRENT DIRECTORY GOOGLE COLAB\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","#!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz\n","#!tar -xvf spark-3.0.0-preview2-bin-hadoop3.2.tgz\n","\n","!wget -q https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop3.2.tgz\n","!tar -xvf spark-3.0.2-bin-hadoop3.2.tgz\n","\n","# INSATLL LIB. FINDSPARK UNTUK MENGIMPORT PYSPARK\n","!pip install -q findspark"],"execution_count":1,"outputs":[{"output_type":"stream","text":["spark-3.0.2-bin-hadoop3.2/\n","spark-3.0.2-bin-hadoop3.2/R/\n","spark-3.0.2-bin-hadoop3.2/R/lib/\n","spark-3.0.2-bin-hadoop3.2/R/lib/sparkr.zip\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/worker/\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/worker/worker.R\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/worker/daemon.R\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/tests/\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/tests/testthat/\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/tests/testthat/test_basic.R\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/profile/\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/profile/shell.R\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/profile/general.R\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/R/\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/R/SparkR\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdx\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdb\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/Meta/\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/Meta/features.rds\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/Meta/package.rds\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/Meta/nsInfo.rds\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/Meta/Rd.rds\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/Meta/links.rds\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/Meta/hsearch.rds\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/DESCRIPTION\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/NAMESPACE\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/html/\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/html/R.css\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/html/00Index.html\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/help/\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/help/aliases.rds\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/help/AnIndex\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdx\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdb\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/help/paths.rds\n","spark-3.0.2-bin-hadoop3.2/R/lib/SparkR/INDEX\n","spark-3.0.2-bin-hadoop3.2/sbin/\n","spark-3.0.2-bin-hadoop3.2/sbin/stop-slaves.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/stop-slave.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/stop-all.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/start-slaves.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/start-slave.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/start-all.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/spark-daemons.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/spark-daemon.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/spark-config.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/slaves.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/stop-thriftserver.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/stop-mesos-shuffle-service.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/stop-mesos-dispatcher.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/stop-master.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/stop-history-server.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/start-thriftserver.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/start-mesos-shuffle-service.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/start-mesos-dispatcher.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/start-master.sh\n","spark-3.0.2-bin-hadoop3.2/sbin/start-history-server.sh\n","spark-3.0.2-bin-hadoop3.2/python/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_utils.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_udf.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_types.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_streaming.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_session.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_serde.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_readwriter.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_window.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_map.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_grouped_map.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_group.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_functions.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_datasources.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_dataframe.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_context.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_conf.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_column.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_catalog.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/tests/test_arrow.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/utils.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/types.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/typehints.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/serializers.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/map_ops.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/functions.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/pandas/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/avro/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/avro/functions.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/avro/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/window.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/utils.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/udf.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/types.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/streaming.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/session.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/group.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/functions.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/context.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/conf.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/column.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/sql/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/rddsampler.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/util.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/tree.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/regression.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/recommendation.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/random.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/fpm.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/feature.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/evaluation.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/common.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/clustering.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/classification.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/tests/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_util.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_streaming_algorithms.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_stat.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_linalg.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_feature.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_algorithms.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/tests/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/stat/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/stat/distribution.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/stat/_statistics.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/stat/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/stat/KernelDensity.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/stat/test.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/linalg/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/linalg/distributed.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/mllib/linalg/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_tuning.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_training_summary.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_stat.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_pipeline.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_persistence.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_param.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_linalg.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_image.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_feature.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_evaluation.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_base.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_algorithms.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_util.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tests/test_wrapper.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/param/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/param/shared.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/param/_shared_params_code_gen.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/param/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/linalg/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/linalg/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/wrapper.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/util.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tuning.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/tree.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/stat.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/regression.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/recommendation.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/pipeline.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/image.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/functions.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/fpm.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/feature.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/evaluation.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/common.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/clustering.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/classification.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/base.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/ml/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/join.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/files.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/daemon.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/_globals.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/worker.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/version.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/util.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/taskcontext.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/storagelevel.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/statcounter.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/shuffle.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/shell.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/serializers.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/resultiterable.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/resource.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/rdd.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/profiler.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/java_gateway.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/heapq3.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/find_spark_home.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/context.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/conf.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/cloudpickle.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/broadcast.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/accumulators.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/traceback_utils.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_worker.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_util.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_taskcontext.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_shuffle.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_serializers.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_readwrite.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_rddbarrier.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_rdd.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_profiler.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_pin_thread.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_join.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_daemon.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_context.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_conf.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_broadcast.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/tests/test_appsubmit.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/testing/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/testing/mllibutils.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/testing/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/testing/utils.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/testing/streamingutils.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/testing/sqlutils.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/testing/mlutils.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/tests/\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/tests/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_listener.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_kinesis.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_dstream.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_context.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/listener.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/__init__.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/util.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/kinesis.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/dstream.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/streaming/context.py\n","spark-3.0.2-bin-hadoop3.2/python/pyspark/status.py\n","spark-3.0.2-bin-hadoop3.2/python/lib/\n","spark-3.0.2-bin-hadoop3.2/python/lib/pyspark.zip\n","spark-3.0.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip\n","spark-3.0.2-bin-hadoop3.2/python/lib/PY4J_LICENSE.txt\n","spark-3.0.2-bin-hadoop3.2/python/docs/\n","spark-3.0.2-bin-hadoop3.2/python/docs/pyspark.streaming.rst\n","spark-3.0.2-bin-hadoop3.2/python/docs/pyspark.sql.rst\n","spark-3.0.2-bin-hadoop3.2/python/docs/pyspark.rst\n","spark-3.0.2-bin-hadoop3.2/python/docs/pyspark.resource.rst\n","spark-3.0.2-bin-hadoop3.2/python/docs/pyspark.mllib.rst\n","spark-3.0.2-bin-hadoop3.2/python/docs/pyspark.ml.rst\n","spark-3.0.2-bin-hadoop3.2/python/docs/make2.bat\n","spark-3.0.2-bin-hadoop3.2/python/docs/index.rst\n","spark-3.0.2-bin-hadoop3.2/python/docs/conf.py\n","spark-3.0.2-bin-hadoop3.2/python/docs/_templates/\n","spark-3.0.2-bin-hadoop3.2/python/docs/_templates/layout.html\n","spark-3.0.2-bin-hadoop3.2/python/docs/_static/\n","spark-3.0.2-bin-hadoop3.2/python/docs/_static/pyspark.js\n","spark-3.0.2-bin-hadoop3.2/python/docs/_static/pyspark.css\n","spark-3.0.2-bin-hadoop3.2/python/docs/_static/copybutton.js\n","spark-3.0.2-bin-hadoop3.2/python/docs/Makefile\n","spark-3.0.2-bin-hadoop3.2/python/docs/make.bat\n","spark-3.0.2-bin-hadoop3.2/python/README.md\n","spark-3.0.2-bin-hadoop3.2/python/.gitignore\n","spark-3.0.2-bin-hadoop3.2/python/.coveragerc\n","spark-3.0.2-bin-hadoop3.2/python/setup.py\n","spark-3.0.2-bin-hadoop3.2/python/run-tests.py\n","spark-3.0.2-bin-hadoop3.2/python/pylintrc\n","spark-3.0.2-bin-hadoop3.2/python/MANIFEST.in\n","spark-3.0.2-bin-hadoop3.2/python/test_support/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/userlib-0.1.zip\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/text-test.txt\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/streaming/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/streaming/text-test.txt\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/people_array_utf16le.json\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/people_array.json\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/people1.json\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/people.json\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_metadata\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_common_metadata\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_SUCCESS\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/_SUCCESS\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/ages_newlines.csv\n","spark-3.0.2-bin-hadoop3.2/python/test_support/sql/ages.csv\n","spark-3.0.2-bin-hadoop3.2/python/test_support/hello/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/hello/sub_hello/\n","spark-3.0.2-bin-hadoop3.2/python/test_support/hello/sub_hello/sub_hello.txt\n","spark-3.0.2-bin-hadoop3.2/python/test_support/hello/hello.txt\n","spark-3.0.2-bin-hadoop3.2/python/test_support/SimpleHTTPServer.py\n","spark-3.0.2-bin-hadoop3.2/python/test_support/userlibrary.py\n","spark-3.0.2-bin-hadoop3.2/python/test_coverage/\n","spark-3.0.2-bin-hadoop3.2/python/test_coverage/sitecustomize.py\n","spark-3.0.2-bin-hadoop3.2/python/test_coverage/coverage_daemon.py\n","spark-3.0.2-bin-hadoop3.2/python/test_coverage/conf/\n","spark-3.0.2-bin-hadoop3.2/python/test_coverage/conf/spark-defaults.conf\n","spark-3.0.2-bin-hadoop3.2/python/setup.cfg\n","spark-3.0.2-bin-hadoop3.2/python/run-tests-with-coverage\n","spark-3.0.2-bin-hadoop3.2/python/run-tests\n","spark-3.0.2-bin-hadoop3.2/bin/\n","spark-3.0.2-bin-hadoop3.2/bin/sparkR2.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/sparkR.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/sparkR\n","spark-3.0.2-bin-hadoop3.2/bin/spark-submit2.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/spark-submit.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/spark-submit\n","spark-3.0.2-bin-hadoop3.2/bin/spark-sql2.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/spark-sql.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/spark-sql\n","spark-3.0.2-bin-hadoop3.2/bin/spark-shell2.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/spark-shell.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/spark-shell\n","spark-3.0.2-bin-hadoop3.2/bin/spark-class.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/spark-class\n","spark-3.0.2-bin-hadoop3.2/bin/run-example.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/run-example\n","spark-3.0.2-bin-hadoop3.2/bin/pyspark.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/beeline.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/beeline\n","spark-3.0.2-bin-hadoop3.2/bin/spark-class2.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/pyspark2.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/pyspark\n","spark-3.0.2-bin-hadoop3.2/bin/load-spark-env.sh\n","spark-3.0.2-bin-hadoop3.2/bin/load-spark-env.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/find-spark-home.cmd\n","spark-3.0.2-bin-hadoop3.2/bin/find-spark-home\n","spark-3.0.2-bin-hadoop3.2/bin/docker-image-tool.sh\n","spark-3.0.2-bin-hadoop3.2/README.md\n","spark-3.0.2-bin-hadoop3.2/conf/\n","spark-3.0.2-bin-hadoop3.2/conf/spark-env.sh.template\n","spark-3.0.2-bin-hadoop3.2/conf/spark-defaults.conf.template\n","spark-3.0.2-bin-hadoop3.2/conf/slaves.template\n","spark-3.0.2-bin-hadoop3.2/conf/metrics.properties.template\n","spark-3.0.2-bin-hadoop3.2/conf/log4j.properties.template\n","spark-3.0.2-bin-hadoop3.2/conf/fairscheduler.xml.template\n","spark-3.0.2-bin-hadoop3.2/data/\n","spark-3.0.2-bin-hadoop3.2/data/streaming/\n","spark-3.0.2-bin-hadoop3.2/data/streaming/AFINN-111.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/streaming_kmeans_data_test.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_svm_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_multiclass_classification_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_movielens_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_linear_regression_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_libsvm_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_lda_libsvm_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_lda_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_kmeans_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_isotonic_regression_libsvm_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_fpgrowth.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/sample_binary_classification_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/ridge-data/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/ridge-data/lpsa.data\n","spark-3.0.2-bin-hadoop3.2/data/mllib/pic_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/pagerank_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/kmeans_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/iris_libsvm.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/grayscale.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA.png\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/license.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/kittens/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/kittens/not-image.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP802813.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP153539.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/kittens/54893.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n","spark-3.0.2-bin-hadoop3.2/data/mllib/images/license.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/gmm_data.txt\n","spark-3.0.2-bin-hadoop3.2/data/mllib/als/\n","spark-3.0.2-bin-hadoop3.2/data/mllib/als/test.data\n","spark-3.0.2-bin-hadoop3.2/data/mllib/als/sample_movielens_ratings.txt\n","spark-3.0.2-bin-hadoop3.2/data/graphx/\n","spark-3.0.2-bin-hadoop3.2/data/graphx/users.txt\n","spark-3.0.2-bin-hadoop3.2/data/graphx/followers.txt\n","spark-3.0.2-bin-hadoop3.2/NOTICE\n","spark-3.0.2-bin-hadoop3.2/licenses/\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-heapq.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-datatables.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-bootstrap.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-zstd.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-zstd-jni.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-xmlenc.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-vis-timeline.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-spire.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-sorttable.js.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-slf4j.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-scopt.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-scala.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-sbt-launch-lib.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-respond.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-reflectasm.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-re2j.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-pyrolite.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-py4j.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-protobuf.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-pmml-model.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-paranamer.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-netlib.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-mustache.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-modernizr.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-minlog.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-matchMedia-polyfill.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-machinist.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-leveldbjni.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-kryo.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-jsp-api.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-json-formatter.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-jquery.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-join.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-jodd.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-jline.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-jaxb-runtime.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-javolution.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-javax-transaction-transaction-api.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-javassist.html\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-janino.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-jakarta.xml.bind-api.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-jakarta.activation-api.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-jakarta-ws-rs-api\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-jakarta-annotation-api\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-istack-commons-runtime.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-graphlib-dot.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-f2j.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-dnsjava.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-dagre-d3.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-d3.min.js.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-cloudpickle.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-automaton.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-arpack.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-antlr.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-JTransforms.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-JLargeArrays.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-CC0.txt\n","spark-3.0.2-bin-hadoop3.2/licenses/LICENSE-AnchorJS.txt\n","spark-3.0.2-bin-hadoop3.2/LICENSE\n","spark-3.0.2-bin-hadoop3.2/examples/\n","spark-3.0.2-bin-hadoop3.2/examples/src/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scripts/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scripts/getGpusResources.sh\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/correlation_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/chisq_selector_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/chi_square_test_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/bucketizer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/bisecting_k_means_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/binarizer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/als_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/aft_survival_regression.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/word2vec_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/vector_slicer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/vector_size_hint_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/vector_indexer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/vector_assembler_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/tokenizer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/tf_idf_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/summarizer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/string_indexer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/stopwords_remover_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/standard_scaler_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/sql_transformer.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/robust_scaler_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/rformula_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/random_forest_regressor_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/random_forest_classifier_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/quantile_discretizer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/polynomial_expansion_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/pipeline_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/train_validation_split.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/prefixspan_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/power_iteration_clustering_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/imputer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/fpgrowth_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/pca_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/onehot_encoder_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/one_vs_rest_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/normalizer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/naive_bayes_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/n_gram_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/multilayer_perceptron_classification.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/min_max_scaler_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/min_hash_lsh_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/max_abs_scaler_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_summary_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/linearsvc.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/lda_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/kmeans_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/isotonic_regression_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/interaction_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/index_to_string_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/generalized_linear_regression_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/gaussian_mixture_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/fm_regressor_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/fm_classifier_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/feature_hasher_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/estimator_transformer_param_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/elementwise_product_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_regression_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_classification_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/dct_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/dataframe_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/cross_validator.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/ml/count_vectorizer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/logistic_regression.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/kmeans.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/avro_inputformat.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/als.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/wordcount.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/transitive_closure.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/status_api_demo.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sort.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/pi.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/parquet_inputformat.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/pagerank.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/streaming/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/streaming/stateful_network_wordcount.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/streaming/sql_network_wordcount.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/streaming/recoverable_network_wordcount.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/streaming/network_wordjoinsentiments.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/streaming/network_wordcount.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/streaming/hdfs_wordcount.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/streaming/queue_stream.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sql/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sql/hive.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sql/datasource.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sql/basic.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sql/arrow.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/word2vec_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/word2vec.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/tf_idf_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/summary_statistics_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/streaming_linear_regression_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/streaming_k_means_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/stratified_sampling_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/standard_scaler_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/sampled_rdds.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/recommendation_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/random_rdd_generation.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_regression_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_classification_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/power_iteration_clustering_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/normalizer_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/naive_bayes_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/kmeans.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/kernel_density_estimation_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/k_means_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/isotonic_regression_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_model.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/elementwise_product_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_regression_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_classification_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/correlations_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/correlations.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/bisecting_k_means_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/binary_classification_metrics_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/svm_with_sgd_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/svd_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/regression_metrics_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/ranking_metrics_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/pca_rowmatrix_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/multi_label_metrics_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/multi_class_metrics_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/python/mllib/fpgrowth_example.py\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/users.parquet\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/users.orc\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/users.avro\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/user.avsc\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/people.txt\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/people.json\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/people.csv\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/kv1.txt\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/full_user.avsc\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/employees.json\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/dir1/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/dir1/file3.json\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/dir1/file1.parquet\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/file2.parquet\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/RSparkSQLExample.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/streaming/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/streaming/structured_network_wordcount.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/svmLinear.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/survreg.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/randomForest.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/prefixSpan.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/powerIterationClustering.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/naiveBayes.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/mlp.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/ml.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/logit.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/lda.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/kstest.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/kmeans.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/isoreg.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/glm.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/gbt.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/gaussianMixture.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/fpm.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/decisionTree.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/bisectingKmeans.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/ml/als.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/dataframe.R\n","spark-3.0.2-bin-hadoop3.2/examples/src/main/r/data-manipulation.R\n","spark-3.0.2-bin-hadoop3.2/examples/jars/\n","spark-3.0.2-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/examples/jars/scopt_2.12-3.7.1.jar\n","spark-3.0.2-bin-hadoop3.2/kubernetes/\n","spark-3.0.2-bin-hadoop3.2/kubernetes/tests/\n","spark-3.0.2-bin-hadoop3.2/kubernetes/tests/worker_memory_check.py\n","spark-3.0.2-bin-hadoop3.2/kubernetes/tests/pyfiles.py\n","spark-3.0.2-bin-hadoop3.2/kubernetes/tests/py_container_checks.py\n","spark-3.0.2-bin-hadoop3.2/kubernetes/dockerfiles/\n","spark-3.0.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/\n","spark-3.0.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/\n","spark-3.0.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/\n","spark-3.0.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n","spark-3.0.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/\n","spark-3.0.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n","spark-3.0.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/entrypoint.sh\n","spark-3.0.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/Dockerfile\n","spark-3.0.2-bin-hadoop3.2/yarn/\n","spark-3.0.2-bin-hadoop3.2/yarn/spark-3.0.2-yarn-shuffle.jar\n","spark-3.0.2-bin-hadoop3.2/jars/\n","spark-3.0.2-bin-hadoop3.2/jars/zstd-jni-1.4.4-3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/zookeeper-3.4.14.jar\n","spark-3.0.2-bin-hadoop3.2/jars/zjsonpatch-0.3.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/xz-1.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/xbean-asm7-shaded-4.15.jar\n","spark-3.0.2-bin-hadoop3.2/jars/woodstox-core-5.0.3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/velocity-1.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/univocity-parsers-2.9.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/transaction-api-1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/token-provider-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/threeten-extra-1.5.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/super-csv-2.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/stream-2.9.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/stax2-api-3.1.4.jar\n","spark-3.0.2-bin-hadoop3.2/jars/stax-api-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spire_2.12-0.17.0-M1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spire-util_2.12-0.17.0-M1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spire-platform_2.12-0.17.0-M1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spire-macros_2.12-0.17.0-M1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-yarn_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-tags_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-tags_2.12-3.0.2-tests.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-streaming_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-sql_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-sketch_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-repl_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-network-shuffle_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-network-common_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-mllib_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-mllib-local_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-mesos_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-launcher_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-kvstore_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-kubernetes_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-hive_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-hive-thriftserver_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-graphx_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-core_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/spark-catalyst_2.12-3.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/snappy-java-1.1.8.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/snakeyaml-1.24.jar\n","spark-3.0.2-bin-hadoop3.2/jars/slf4j-log4j12-1.7.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/slf4j-api-1.7.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/shims-0.7.45.jar\n","spark-3.0.2-bin-hadoop3.2/jars/shapeless_2.12-2.3.3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/scala-xml_2.12-1.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/scala-reflect-2.12.10.jar\n","spark-3.0.2-bin-hadoop3.2/jars/scala-parser-combinators_2.12-1.1.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/scala-library-2.12.10.jar\n","spark-3.0.2-bin-hadoop3.2/jars/scala-compiler-2.12.10.jar\n","spark-3.0.2-bin-hadoop3.2/jars/scala-collection-compat_2.12-2.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/re2j-1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/pyrolite-4.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/py4j-0.10.9.jar\n","spark-3.0.2-bin-hadoop3.2/jars/protobuf-java-2.5.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/parquet-jackson-1.10.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/parquet-hadoop-1.10.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/parquet-format-2.4.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/parquet-encoding-1.10.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/parquet-common-1.10.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/parquet-column-1.10.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/paranamer-2.8.jar\n","spark-3.0.2-bin-hadoop3.2/jars/osgi-resource-locator-1.0.3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/oro-2.0.8.jar\n","spark-3.0.2-bin-hadoop3.2/jars/orc-shims-1.5.10.jar\n","spark-3.0.2-bin-hadoop3.2/jars/orc-mapreduce-1.5.10.jar\n","spark-3.0.2-bin-hadoop3.2/jars/orc-core-1.5.10.jar\n","spark-3.0.2-bin-hadoop3.2/jars/opencsv-2.3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/okio-1.15.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/okhttp-3.12.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/okhttp-2.7.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/objenesis-2.5.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/nimbus-jose-jwt-4.41.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/netty-all-4.1.47.Final.jar\n","spark-3.0.2-bin-hadoop3.2/jars/minlog-1.3.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/metrics-jvm-4.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/metrics-json-4.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/metrics-jmx-4.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/metrics-graphite-4.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/metrics-core-4.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/mesos-1.4.0-shaded-protobuf.jar\n","spark-3.0.2-bin-hadoop3.2/jars/macro-compat_2.12-1.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/machinist_2.12-0.6.8.jar\n","spark-3.0.2-bin-hadoop3.2/jars/lz4-java-1.7.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/logging-interceptor-3.12.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/log4j-1.2.17.jar\n","spark-3.0.2-bin-hadoop3.2/jars/libthrift-0.12.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/libfb303-0.9.3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/leveldbjni-all-1.8.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kubernetes-model-common-4.9.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kubernetes-model-4.9.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kubernetes-client-4.9.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kryo-shaded-4.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerby-xdr-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerby-util-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerby-pkix-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerby-config-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerby-asn1-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerb-util-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerb-simplekdc-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerb-server-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerb-identity-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerb-crypto-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerb-core-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerb-common-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerb-client-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/kerb-admin-1.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jul-to-slf4j-1.7.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jta-1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jsr305-3.0.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jsp-api-2.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/json4s-scalap_2.12-3.6.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/json4s-jackson_2.12-3.6.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/json4s-core_2.12-3.6.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/json4s-ast_2.12-3.6.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/json-smart-2.3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/json-1.8.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jpam-1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jodd-core-3.5.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/joda-time-2.10.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jline-2.14.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jersey-server-2.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jersey-media-jaxb-2.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jersey-hk2-2.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jersey-container-servlet-core-2.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jersey-container-servlet-2.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jersey-common-2.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jersey-client-2.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jdo-api-3.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jcl-over-slf4j-1.7.30.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jcip-annotations-1.0-1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jaxb-runtime-2.3.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jaxb-api-2.2.11.jar\n","spark-3.0.2-bin-hadoop3.2/jars/javolution-5.5.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/javax.servlet-api-3.1.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/javax.jdo-3.2.0-m3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/javax.inject-1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/javassist-3.25.0-GA.jar\n","spark-3.0.2-bin-hadoop3.2/jars/janino-3.0.16.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jakarta.xml.bind-api-2.3.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jakarta.ws.rs-api-2.1.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jakarta.validation-api-2.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jakarta.inject-2.6.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jakarta.annotation-api-1.3.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jakarta.activation-api-1.2.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-module-scala_2.12-2.10.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-module-paranamer-2.10.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-module-jaxb-annotations-2.10.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-mapper-asl-1.9.13.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-jaxrs-json-provider-2.9.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-jaxrs-base-2.9.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-datatype-jsr310-2.10.3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-dataformat-yaml-2.10.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-databind-2.10.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-core-asl-1.9.13.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-core-2.10.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/jackson-annotations-2.10.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/ivy-2.4.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/istack-commons-runtime-3.0.8.jar\n","spark-3.0.2-bin-hadoop3.2/jars/httpcore-4.4.12.jar\n","spark-3.0.2-bin-hadoop3.2/jars/httpclient-4.5.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/htrace-core4-4.1.0-incubating.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hk2-utils-2.6.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hk2-locator-2.6.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hk2-api-2.6.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-vector-code-gen-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-storage-api-2.7.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-shims-scheduler-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-shims-common-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-shims-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-shims-0.23-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-serde-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-metastore-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-llap-common-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-jdbc-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-exec-2.3.7-core.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-common-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-cli-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hive-beeline-2.3.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-yarn-server-web-proxy-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-yarn-server-common-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-yarn-registry-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-yarn-common-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-yarn-client-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-yarn-api-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-mapreduce-client-jobclient-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-mapreduce-client-core-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-mapreduce-client-common-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-hdfs-client-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-common-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-client-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-auth-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/hadoop-annotations-3.2.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/guice-servlet-4.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/guice-4.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/guava-14.0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/gson-2.2.4.jar\n","spark-3.0.2-bin-hadoop3.2/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/generex-1.0.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/flatbuffers-java-1.9.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/ehcache-3.3.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/dnsjava-2.1.7.jar\n","spark-3.0.2-bin-hadoop3.2/jars/derby-10.12.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/datanucleus-rdbms-4.1.19.jar\n","spark-3.0.2-bin-hadoop3.2/jars/datanucleus-core-4.1.17.jar\n","spark-3.0.2-bin-hadoop3.2/jars/datanucleus-api-jdo-4.2.4.jar\n","spark-3.0.2-bin-hadoop3.2/jars/curator-recipes-2.13.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/curator-framework-2.13.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/curator-client-2.13.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/core-1.1.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/compress-lzf-1.0.3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-text-1.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-pool-1.5.4.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-net-3.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-math3-3.4.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-logging-1.1.3.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-lang3-3.9.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-lang-2.6.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-io-2.4.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-httpclient-3.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-dbcp-1.4.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-daemon-1.0.13.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-crypto-1.1.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-configuration2-2.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-compress-1.20.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-compiler-3.0.16.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-collections-3.2.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-codec-1.10.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-cli-1.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/commons-beanutils-1.9.4.jar\n","spark-3.0.2-bin-hadoop3.2/jars/chill_2.12-0.9.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/chill-java-0.9.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/cats-kernel_2.12-2.0.0-M4.jar\n","spark-3.0.2-bin-hadoop3.2/jars/breeze_2.12-1.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/breeze-macros_2.12-1.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/bonecp-0.8.0.RELEASE.jar\n","spark-3.0.2-bin-hadoop3.2/jars/avro-mapred-1.8.2-hadoop2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/avro-ipc-1.8.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/avro-1.8.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/automaton-1.11-8.jar\n","spark-3.0.2-bin-hadoop3.2/jars/audience-annotations-0.5.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/arrow-vector-0.15.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/arrow-memory-0.15.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/arrow-format-0.15.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/arpack_combined_all-0.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/aopalliance-repackaged-2.6.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/aopalliance-1.0.jar\n","spark-3.0.2-bin-hadoop3.2/jars/antlr4-runtime-4.7.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/antlr-runtime-3.5.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/algebra_2.12-2.0.0-M2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/aircompressor-0.10.jar\n","spark-3.0.2-bin-hadoop3.2/jars/activation-1.1.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/accessors-smart-1.2.jar\n","spark-3.0.2-bin-hadoop3.2/jars/ST4-4.0.4.jar\n","spark-3.0.2-bin-hadoop3.2/jars/RoaringBitmap-0.7.45.jar\n","spark-3.0.2-bin-hadoop3.2/jars/JTransforms-3.1.jar\n","spark-3.0.2-bin-hadoop3.2/jars/JLargeArrays-1.5.jar\n","spark-3.0.2-bin-hadoop3.2/jars/HikariCP-2.5.1.jar\n","spark-3.0.2-bin-hadoop3.2/RELEASE\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N8FvRJAgNWW6","executionInfo":{"status":"ok","timestamp":1614598280280,"user_tz":-420,"elapsed":27235,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}}},"source":["# IMPORT LIB. OS UNTUK MEN-SETTING ENVIRONMENT GOOGLE COLAB\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview2-bin-hadoop3.2\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop3.2\""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HG-qjF7qNFKn"},"source":["# IMPORT BEBERAPA LIBRARY "]},{"cell_type":"code","metadata":{"id":"RT95wikuOgGt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614598283616,"user_tz":-420,"elapsed":21484,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}},"outputId":"d416c497-d77d-4783-ba38-8c6327411ef0"},"source":["# INSTALL LIB. SASTRAWI UNTUK DIGUNAKAN PADA STEMMING\n","!pip install Sastrawi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting Sastrawi\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4b/bab676953da3103003730b8fcdfadbdd20f333d4add10af949dd5c51e6ed/Sastrawi-1.0.1-py2.py3-none-any.whl (209kB)\n","\r\u001b[K     |                              | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |                            | 20kB 23.3MB/s eta 0:00:01\r\u001b[K     |                           | 30kB 20.7MB/s eta 0:00:01\r\u001b[K     |                         | 40kB 15.7MB/s eta 0:00:01\r\u001b[K     |                        | 51kB 9.3MB/s eta 0:00:01\r\u001b[K     |                      | 61kB 10.5MB/s eta 0:00:01\r\u001b[K     |                     | 71kB 11.7MB/s eta 0:00:01\r\u001b[K     |                   | 81kB 12.8MB/s eta 0:00:01\r\u001b[K     |                  | 92kB 13.9MB/s eta 0:00:01\r\u001b[K     |                | 102kB 12.6MB/s eta 0:00:01\r\u001b[K     |              | 112kB 12.6MB/s eta 0:00:01\r\u001b[K     |             | 122kB 12.6MB/s eta 0:00:01\r\u001b[K     |           | 133kB 12.6MB/s eta 0:00:01\r\u001b[K     |          | 143kB 12.6MB/s eta 0:00:01\r\u001b[K     |        | 153kB 12.6MB/s eta 0:00:01\r\u001b[K     |       | 163kB 12.6MB/s eta 0:00:01\r\u001b[K     |     | 174kB 12.6MB/s eta 0:00:01\r\u001b[K     |   | 184kB 12.6MB/s eta 0:00:01\r\u001b[K     |  | 194kB 12.6MB/s eta 0:00:01\r\u001b[K     || 204kB 12.6MB/s eta 0:00:01\r\u001b[K     || 215kB 12.6MB/s \n","\u001b[?25hInstalling collected packages: Sastrawi\n","Successfully installed Sastrawi-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rTtdtmUjNFKs","executionInfo":{"status":"ok","timestamp":1614598283620,"user_tz":-420,"elapsed":19346,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}}},"source":["import pandas as pd   #--> LIB. PANDAS UNTUK MEMUAT DATAFRAME, TETAPI TIDAK DIGUNAKAN UNTUK PENGOLAHAN PADA ANALISIS SENTIMEN INI\n","import io as io       #--> LIB. IO UNTUK MEMBACA FILE INPUT YANG DIUPLAD KE DALAM GOOGLE COLAB\n","import requests as rq #--> LIB. REQUEST UNTUK MENGAMBIL STOPLIST TALA\n","import math           #--> LIB. MATH UNTUK MENGHITUNG LOG10 SAAT MENENTUKAN TERM FREQUENCY\n","import Sastrawi       #--> LIB. SASTRAWI UNTUK MEN-STEMMING TERM\n","import time           #--> LIB. TIME UNTUK MENGHITUNG RUNTIME EXCECUTION\n","\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  #--> LIB. STEMMERFACTORY MERUPAKAN FUNCTION STEMMING \n","from google.colab import files                              #--> LIB. FILES UNTUK MENGUPLOAD FILE DATASET KE DALAM GOOGLE COLAB"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"HVmXfRn4NFLS","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1614598283623,"user_tz":-420,"elapsed":18414,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}},"outputId":"a51170fb-5fbc-4bdb-a985-babb81b7557a"},"source":["import findspark  #--> LIB. FINDSPARK UNTUK MENGAMBIL SPARK PADA DIRECTORY GOOGLE COLAB\n","findspark.init()  #--> FUNCTION MENGINISIASI SPARK PADA DIRECTORY GOOGLE COLAB\n","findspark.find()  #--> MENAMPILKAN LOKASI SPARK PADA DIRECTORY GOOGLE COLAB"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/spark-3.0.2-bin-hadoop3.2'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"aGB_bYC7NFLa","executionInfo":{"status":"ok","timestamp":1614598288328,"user_tz":-420,"elapsed":20961,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}}},"source":["import pyspark  #--> LIB. PYSPARK SEBAGAI OPERASI RDD PADA ANALISIS SENTIMEN INI\n","\n","from pyspark.sql import SparkSession                         #--> LIB. SPARKSESSION UNTUK MENJALANKAN PYSPARK\n","from pyspark.sql.functions import explode, concat, col, lit  #--> BEBERAPA LIB DARI PYSPARK UNTUK DIGUNAKAN\n","\n","spark = SparkSession.builder.getOrCreate()  #--> MENJALANKAN PYSPARK\n","sc = spark.sparkContext"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NykVihxiNFLr"},"source":["# MEMISAHKAN DATA LATIH & DATA UJI"]},{"cell_type":"markdown","metadata":{"id":"N0rMBpfjNFME"},"source":["## 1) MEMBUAT DATA LATIH"]},{"cell_type":"code","metadata":{"id":"C67PQwFyNFMF"},"source":["def get_DF_DataLatih(dfNegatif, dfPositif):\n","  # MENGAMBIL DATA KELAS POSITIF & NEGATIF SECARA MERATA UNTUK DIJADIKAN DATA LATIH\n","  dfLatihPositif = spark.createDataFrame(dfPositif.take(int(drainData/2)))\n","  dfLatihNegatif = spark.createDataFrame(dfNegatif.take(int(drainData/2)))\n","  \n","  dfDataLatih = dfLatihNegatif.union(dfLatihPositif)  #--> MENGGABUNGKAN DATA KELAS POSITIF & NEGATIF YANG SUDAH DIBAGI RATA\n","\n","  return dfDataLatih\n","  #dfDataLatih.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XufwK75rd6Ay"},"source":["def get_DF_DataLatih_KFOLD(dfDataset, dfDataUji):\n","  # MENGAMBIL DATA UJI DENGAN MEMBANDINGAKAN DATASET & DATA LATIH\n","  dfDataLatih = dfDataset.join(dfDataUji, [\"Judul Berita\", \"Isi Berita\", \"Kelas Berita\"], \"leftanti\") #--> MENGAMBIL DATA YANG BERBEDA DENGAN MENGGUNAKAN \"LEFTANTI\"\n","  dfDataLatih = dfDataLatih.sort(\"Kelas Berita\", ascending=True)                                          #--> MENGURUTKAN DATA UJI BERDASARKAN NAMA KELAS \n","\n","  return dfDataLatih\n","  #dfDataLatih.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fksZnOhh48Oa"},"source":["def get_RDD_DataLatih(dfDataLatih):  \n","  df = dfDataLatih.select(col(\"Unnamed: 0\"), col(\"Judul Berita\"), col(\"Isi Berita\")) #--> MEMILIH INDEKS BERITA, JUDUL BERITA, DAN ISI BERITA\n","  rddDataLatih = df.rdd.map(lambda x: (\"D\"+str(x[0]), str(x[1]+\" \"+x[2])))           #--> MENYIMPAN DATA LATIH KE DALAM SPARK RDD\n","\n","  return rddDataLatih"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yJENjYYWNFMN"},"source":["## 2) MEMBUAT DATA UJI"]},{"cell_type":"code","metadata":{"id":"j_5TXCWVNFMO"},"source":["def get_DF_DataUji(dfDataset, dfDataLatih):\n","  # MENGAMBIL DATA UJI DENGAN MEMBANDINGAKAN DATASET & DATA LATIH\n","  dfDataUji = dfDataset.join(dfDataLatih, [\"Judul Berita\", \"Isi Berita\", \"Kelas Berita\"], \"leftanti\") #--> MENGAMBIL DATA YANG BERBEDA DENGAN MENGGUNAKAN \"LEFTANTI\"\n","  dfDataUji = dfDataUji.sort(\"Kelas Berita\", ascending=True)                                          #--> MENGURUTKAN DATA UJI BERDASARKAN NAMA KELAS \n","\n","  return dfDataUji\n","  #dfDataUji.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOBVmA5kdKsz"},"source":["def get_DF_DataUji_KFOLD(dfNegatif, dfPositif):\n","  dfUjiNegatif = spark.createDataFrame(dfNegatif)\n","  dfUjiPositif = spark.createDataFrame(dfPositif)\n","\n","  dfDataUji = dfUjiNegatif.union(dfUjiPositif)\n","\n","  return dfDataUji\n","  #dfDataUji.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VRUTPAav51iH"},"source":["def get_RDD_DataUji(dfDataUji):  \n","  df = dfDataUji.select(col(\"Unnamed: 0\"), col(\"Judul Berita\"), col(\"Isi Berita\")) #--> MEMILIH INDEKS BERITA, JUDUL BERITA, DAN ISI BERITA\n","  rddDataUji = df.rdd.map(lambda x: (\"D\"+str(x[0]), str(x[1]+\" \"+x[2])))           #--> MENYIMPAN DATA UJI KE DALAM SPARK RDD\n","\n","  return rddDataUji"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rm-t1R-lNFMW"},"source":["# TEXT PREPROCESSING"]},{"cell_type":"markdown","metadata":{"id":"ci4EWVFn3Rsv"},"source":["##1) TOKENIZING"]},{"cell_type":"code","metadata":{"id":"IANWJpu_YMjd"},"source":["# MEMBUAT FUNCTION TOKENIZE UNTUK MEMISAHKAN SETIAP KATA PADA DATA \n","def Tokenize(rddValue):\n","  punc_Num='!\"#$%&\\'()*+,:;<=>?@[\\\\]^_`{|}~0123456789'\n","  for ch in punc_Num:                           #--> PERULANGAN UNTUK MENGECEK CHARACTER BERDASARKAN \"punc_Num\"\n","    rddValue = rddValue.replace(ch, \"\")         #--> MENGHAPUS CHARACTER \n","  rddValue = rddValue.replace(\".\", \" \")         #--> MENGGANTIKAN CHARACTER \".\" MENJADI WHITESPACE \" \"\n","  rddValue = rddValue.replace(\"-\", \" \")         #--> MENGGANTIKAN CHARACTER \"-\" MENJADI WHITESPACE \" \"\n","  rddValue = rddValue.replace(\"/\", \" \")         #--> MENGGANTIKAN CHARACTER \"/\" MENJADI WHITESPACE \" \"\n","  rddValue = rddValue.replace(\"\", \" \")         #--> MENGGANTIKAN CHARACTER \",\" MENJADI WHITESPACE \" \"\n","  rddValue = rddValue.replace(\"\\xa0\", \" \")      #--> MENGGANTIKAN CHARACTER \"\\xa0\" MENJADI WHITESPACE \" \"\n","  rddValue = rddValue.replace(\"\\x80\\x9c\", \" \")  #--> MENGGANTIKAN CHARACTER \"\\x80\\x9c\" MENJADI WHITESPACE \" \"\n","  rddValue = rddValue.replace(\"\\x80\", \" \")      #--> MENGGANTIKAN CHARACTER \"\\x80\" MENJADI WHITESPACE \" \"\n","  rddValue = rddValue.replace(\"\\x93\", \" \")      #--> MENGGANTIKAN CHARACTER \"\\x93\" MENJADI WHITESPACE \" \"\n","  rddValue = rddValue.replace(\"\\x94\", \" \")      #--> MENGGANTIKAN CHARACTER \"\\x94\" MENJADI WHITESPACE \" \"\n","  rddValue = rddValue.replace(\"\\x99\", \" \")      #--> MENGGANTIKAN CHARACTER \"\\x99\" MENJADI WHITESPACE \" \"\n","\n","  rddToken = rddValue.split()   #--> MEMISAHKAN SETIAP KATA\n","  \n","  return rddToken"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oqxNfD4gNFMY"},"source":["def Tokenizing(rddTextPrep):\n","  # PROSES MAPPING UNTUK MELAKUKAN TOKENIZING\n","  rddToken = rddTextPrep.map(lambda x: (x[0], Tokenize(x[1])))              #--> PROSES INISIALSASI (D+INDEKS) SEBAGAI KEY, TOKENIZING SEBAGAI VALUE)\n","  #rddToken = rddTextPrep.map(lambda x: (\"D\"+str(x[1]+1), Tokenize(x[0])))  #--> PROSES INISIALSASI (D+INDEKS) SEBAGAI KEY, TOKENIZING SEBAGAI VALUE)\n","\n","  print(\"  Tokenizing Done. . .\")\n","  return rddToken"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YN6s0-S23Z85"},"source":["##2) FILTERING"]},{"cell_type":"code","metadata":{"id":"N2M3YdVKVlX8"},"source":["def Filtering(rddToken):\n","  # MEMBUAT STOPLIST UNTUK FILTERING\n","  #stopList = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"] \n","  stopList = []\n","  listBerita = [\"cnn\", \"cnnindonesia\", \"cnnindonesiacom\", \"detik\", \"detikcom\", \"liputan\", \"liputancom\", \"inipasti\", \"inipasticom\", \"antaranews\", \"antaranewscom\", \"com\"]\n","\n","  # MENGAMBIL STOPLIST TALA\n","  url = \"http://static.hikaruyuuki.com/wp-content/uploads/stopword_list_tala.txt\" #--> LINK STOPLIST TALA\n","  headers = {'Accept-Encoding': 'identity'}\n","  r = rq.get(url, headers=headers); ind = 0;  #--> REQUEST LINK UNTUK MENGAMBIL STOPLIST TALA\n","  for line in r.text.splitlines():            #--> PERULANGAN UNTUK MENG-APPEND STOPLIST TALA KE DALAM \"stopList\"\n","    #print(line)\n","    stopList.append(line)\n","    ind += 1\n","    if len(r.text.splitlines()) == ind:     #--> MELAKUKAN PENAMBAHAN STOPLIST DARI \"listBerita\" APABILA SUDAH PADA PERULANGAN TERAKHIR\n","      for berita in listBerita:             #--> PERULANGAN UNTUK MENG-APPEND STOPLIST \"listBerita\" KE DALAM \"stopList\"\n","        stopList.append(berita)\n","\n","  # PEOSES MAPPTING UNTK MELAKUKAN FILTERING\n","  rddFilter = rddToken.map(lambda x: (x[0], [item for item in x[1] if item.lower() not in stopList]))\n","\n","  print(\"  Filtering Done. . .\")\n","  return rddFilter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-MqloTERwtQ"},"source":["##3) STEMMING"]},{"cell_type":"code","metadata":{"id":"pbTUxhIWR0HB"},"source":["def Stemming(rddFilter):\n","  # MEMBUAT STEMLIST DARI FUNCTION STEMMERFACTORY\n","  factory = StemmerFactory()\n","  stemmer = factory.create_stemmer()\n","\n","  # PROSES MAPPING UNTUK MELAKUKAN STEMMING\n","  rddStem = rddFilter.map(lambda x: (x[0], [stemmer.stem(item) for item in x[1]]))\n","\n","  print(\"  Stemming Done. . .\")\n","  return rddStem"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itG8RGbuTfLf"},"source":["#TERM WEIGHTING"]},{"cell_type":"markdown","metadata":{"id":"4Ta1jttPVjVd"},"source":["##1) MENGHITUNG TF"]},{"cell_type":"code","metadata":{"id":"QWN84HXlwWdM"},"source":["def hitungTF(rddTerm):\n","  # MENGHITUNG TERM FREQUENCY DENGAN LOG-TF\n","  rddTF = rddTerm.flatMap(lambda x: [((x[0][0], item), 1) for item in x[1]])  #--> PROSES MAP MENGAMBIL VALUE DARI \"rddTerm\" DENGAN (DOK. INDEKS, TERM), 1 SEBAGAI VALUE\n"," \n","  rddTF = rddTF.reduceByKey(lambda x, y: x + y)                               #--> PROSES REDUCE MENJUMLAHKAN TOTAL VALUE SESUAI KEY YANG SAMA\n","  rddTF = rddTF.map(lambda x: (x[0], 1+math.log10(x[1])))                     #--> PROSES MAP (DOK. INDEKS, TERM) SEBAGAI KEY, PERHITUNGAN LOG-TF SEBAGAI VALUE\n","\n","  print(\"  TF Done. . .\")\n","  return rddTF"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TeipEfrU3oHO"},"source":["##2) MENGHITUNG DF"]},{"cell_type":"code","metadata":{"id":"6HDnMXXl3s63"},"source":["def hitungDF(rddTerm):\n","  # ME-REDUCE KATA PADA MASING-MASING DOKUMEN\n","  rddDF = rddTerm.flatMap(lambda x: [((x[0][0], item), 1) for item in x[1]])   #--> PROSES MAP MENGAMBIL VALUE DARI \"rddTerm\" DENGAN (DOK. INDEKS, TERM), 1 SEBAGAI VALUE\n","  rddDF = rddDF.reduceByKey(lambda x, y: 1)                                    #--> PROSES REDUCE MERUBAH VALUE MENJADI 1 SESUAI KEY YANG SAMA PADA MASING-MASING DOK.\n","\n","  # ME-REDUCE KATA PADA SELURUH DOKUMEN\n","  rddDF = rddDF.map(lambda x: (x[0][1], 1))         #--> PROSES MAP TERM SEBAGAI KEY, 1 SEBAGAI VALUE\n","  rddDF = rddDF.reduceByKey(lambda x, y: (x + y))   #--> PROSES REDUCE MENJUMLAHKAN VALUE SESUAI KEY YANG SAMA   \n","\n","  print(\"  DF Done. . .\")\n","  return rddDF"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_LgHlGJf63Gu"},"source":["##3) MENGHITUNG IDF"]},{"cell_type":"code","metadata":{"id":"HS0SCrI165sO"},"source":["def hitungIDF(rddDF, countDoc):\n","  # MENGHITUNG IDF\n","  rddIDF = rddDF.map(lambda x: (x[0], math.log10(countDoc/x[1])))   #--> PROSES MAP TERM SEBAGAI KEY, PERHITUNGAN IDF SEBAGAI VALUE\n","\n","  print(\"  IDF Done. . .\")\n","  return rddIDF"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RN_zLXa68L8Q"},"source":["##4) MENGHITUNG TF-IDF"]},{"cell_type":"code","metadata":{"id":"agZznOMh8akZ"},"source":["def hitungTFIDF(rddTF, rddIDF, rddKelasDataLatih):\n","  # MENYIAPKAN RDD UNTUK MENGHITUNG TF-IDF\n","  rddTFIDF = rddTF.map(lambda x: (x[0][1], (x[0][0], x[1])))          #--> PROSES MAP TERM SEBAGAI KEY, (DOC. INDEKS, TF) SEBAGAI VALUE\n","  rddTFIDF = rddTFIDF.join(rddIDF)                                    #--> PROSES MENGGABUNGKAN DENGAN \"rddIDF\"\n","  rddTFIDF = rddTFIDF.mapValues(lambda x: (x[0][0], x[0][1], x[1]))   #--> PROSES MAP TERM SEBAGAI KEY, (DOK. INDEKS, TF, IDF) SEBAGAI KEY\n","\n","  # MENGHITUNG TF-IDF\n","  rddTFIDF = rddTFIDF.map(lambda x: (x[1][0], (x[0], x[1][1]*x[1][2]))) #--> PROSES MAP DOC. INDEKS SEBAGAI KEY, (TERM, TF-IDF) SEBAGAI VALUE\n","\n","  # MENGAMBIL KELAS DARI SETIAP DATA LATIH\n","  rddTFIDF = rddTFIDF.join(rddKelasDataLatih)                                     #--> PROSES MENGGABUNGKAN DENGAN \"rddKelasDataLatih\", MENGAMBIl KELAS DARI DATA LATIH\n","  rddTFIDF = rddTFIDF.map(lambda x: ((x[0], x[1][0][0], x[1][1]), x[1][0][1]))    #--> PROSES MAP (DOC. INDEKS, TERM, KELAS) SEBAGAI KEY, TF-IDF SEBAGAI VALUE\n","  \n","  print(\"  TFIDF Done. . .\")\n","  return rddTFIDF"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"StqC3ASP8Gen"},"source":["#KLASIFIKASI"]},{"cell_type":"markdown","metadata":{"id":"4SWUxTS-euUW"},"source":["##1) MENGHITUNG PRIOR"]},{"cell_type":"code","metadata":{"id":"D6Z_vkDs8DB3"},"source":["# rddKelasDataLatih = (D1, Positif), (D2, Positif), (D3, Negatif), (D4, Negatif)\n","# rddPrior = (Positif, 1), (Positif, 1), (Negatif, 1), (Negatif, 1)\n","\n","\n","\n","def hitungPrior(rddKelasDataLatih, countDoc)\n","  # MENGAMBIL KELAS UNTUK MENGHITUNG PRIOR\n","  rddPrior = rddKelasDataLatih.map(lambda x: x[1])  #--> PROSES MAP UNTUK MENGAMBIL KELAS SEBAGAI VALUE\n","\n","  # MENGHITUNG PRIOR\n","  rddPrior = rddPrior.map(lambda x: (x, 1))         #--> PROSES MAP KELAS SEBAGAI KEY, 1 SEBAGAI VALUE\n","  rddPrior = rddPrior.reduceByKey(lambda x, y: x + y).map(lambda x : (x[0], x[1]/countDoc))  #--> PRESES REDUCE MENGHITUNG VALUE DAN DI-MAP UNTUK MEMBAGI VALUE DENGAN TOTAL DATA LATIH\n","\n","  print(\"  Prior Done. . .\")\n","  return rddPrior\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F8HPXgG_lEtN"},"source":["##2) MENGHITUNG LIKELIHOOD"]},{"cell_type":"code","metadata":{"id":"mMRAmcZZlckm"},"source":["def hitungLikelihood(rddIDF, rddTFIDF, rddStem, rddDataUji, countTerm):       \n","  # MENGAMBIL NILAI IDF UNTUK MEMISAHKAN TERM MASING-MASING KELAS\n","  rddTermClass = rddIDF.map(lambda x: (x[0], [\"Negatif\", \"Positif\"]))              #--> PROSES MAP TERM SEBAGAI KEY, (NEGATIF, POISITF) SEBAGAI VALUE DARI \"rddIDF\"\n","  rddLikelihood = rddTermClass.flatMap(lambda x: [(item, x[0]) for item in x[1]])  #--> PROSES MAP MEMISAHKAN SETIAP KELAS, KELAS SEBAGAI KEY, TERM SEBAGAI VALUE\n","\n","  # MENGHITUNG/COUNT SELURUH TW DARI MASING-MASING KELAS\n","  rddTotalTermClass = rddTFIDF.map(lambda x: (x[0][2], x[1]))             #--> PROSES MAP KELAS SEBAGAI KEY, TF-IDF SEBAGAI VALUE\n","  rddTotalTermClass = rddTotalTermClass.reduceByKey(lambda x, y: x + y)   #--> PROSES REDUCE MENJUMLAHKAN VALUE SESUAI KEY YANG SAMA\n","\n","  # MENGGABUNGKAN HASIL COUNT SELURUH TW KE DALAM \"rddLikelihood\"\n","  rddLikelihood = rddLikelihood.join(rddTotalTermClass).map(lambda x: ((x[1][0], x[0]), x[1][1])) #--> MENGGABUNGKAN DENGAN \"rddTotalTermClass\" KEMUDIAN MELAKUKAN MAPPING (TERM, KELAS) SEBAGAI KEY, COUNT TW SEBAGAI VALUE\n","  \n","  # MENGHIUTNG/COUNT TERM TW DARI MASING-MASING KELAS\n","  rddTotalTermEachClass = rddTermClass.flatMap(lambda x: [((x[0], item), 0) for item in x[1]])    #--> PROSES MAP MEMISAHKAN SETIAP KELAS, (TERM, KELAS) SEBAGAI KEY, 0 SEBAGAI VALUE\n","\n","  rddT_TFIDF_Term = rddTFIDF.map(lambda x: ((x[0][1], x[0][2]), x[1]))                                    #--> PROSES MAP (TERM, KELAS) SEBAGAI KEY, TF-IDF SEBAGAI VALUE DARI \"rddTFIDF\"\n","  rddT_TFIDF_Term = rddT_TFIDF_Term.reduceByKey(lambda x, y: x + y)                                       #--> PROSES REDUCE MENJUMLAHKAN VALUE SESUAI KEY YANG SAMA\n","\n","  rddTotalTermEachClass = rddTotalTermEachClass.leftOuterJoin(rddT_TFIDF_Term)                            #--> MENGGABUNGKAN RDD UNTUK MENGAMBIL COUNT TERM TW DARI MASING-MASING KELAS\n","  rddTotalTermEachClass = rddTotalTermEachClass.mapValues(lambda x: x[1] if not x[1] is None else 0)      #--> MENGHITUNG COUNT TERM TW, APABILA VALUE \"None\" BERARTI COUNT TERM BERNILAI 0\n","\n","  # MENGGABUNGKAN HASIL COUNT TERM TW KE DALAM \"rddLikelihood\"\n","  rddLikelihood = rddLikelihood.join(rddTotalTermEachClass).map(lambda x: (x[0][0], x[0][1], x[1][0], x[1][1]))    #--> MENGGABUNGKAN DENGAN \"rddTotalTermEachClass\" KEMUDIAN MELAKUKAN MAPPING (TERM, KELAS, COUNT TW, COUNT TERM TW) SEBAGAI VALUE\n","  ###rddLikelihood = rddLikelihood.reduceByKey(lambda k, v: k + v)                                                 #--> PROSES REDUCE MENJUMLAHKAN VALUE SESUAI KEY YANG SAMA\n","\n","  # MELAKUKAN MAPPING \"rddLikelihood\" UNTUK MENGAMBIL NILAI TOTAL TERM\n","  rddLikelihood = rddLikelihood.map(lambda x: (\"[KEY_COUNT_TERM]\", x))    #--> PROSES MAP [KEY_COUNT_TERM] SEBAGAI KEY, (TERM, KELAS, COUNT TW, COUNT TERM TW) SEBAGAI VALUE\n","  rddLikelihood = countTerm.join(rddLikelihood)                           #--> MENGGABUNGKAN RDD UNTUK MENGAMBIL TOTAL TERM DATA LATIH DARI \"countTerm\"\n","\n","  # MENGHITUNG MULTINOMIAL NAIVE BAYES (LIKELIHOOD)\n","  rddLikelihood = rddLikelihood.map(lambda x: (x[1][1][0], (x[1][1][1], (x[1][1][3] + 1)/(x[1][1][2] + x[1][0])))) \n","\n","  rddLikelihoodUji = hitungLikelihoodUji(rddStem, rddLikelihood, rddDataUji)\n","\n","  print(\"  Likelihood Done. . .\")\n","  return rddLikelihoodUji"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A94UVp_B5eZd"},"source":["def hitungLikelihoodUji(rddStem, rddLikelihood, rddDataUji):\n","  # MEMPERSIAPKAN TERM DATA UJI\n","  rddStemDataUji = rddStem.join(rddDataUji)  #--> MENGAMBIL DATA UJI DENGAN MENGGABUNGKAN SESUAI KEY YANG ADA PADA \"rddStem\" & \"rddStemDataUji\"\n","\n","  #PROSES MAPPING UNTUK MERUBAH KEY DAN VALUE \"rddStemDataUji\"\n","  rddStemDataUji = rddStemDataUji.map(lambda x: (\"Q\"+str(int(x[0][1:])), x[1][0]))  #--> QUERY INDEKS SEBAGAI KEY, TERM LIST SEBAGAI VALUE\n","\n","  # MENJADIKAN TERM SEBAGAI DATA RDD\n","  rddTermUji = rddStemDataUji.flatMap(lambda x: [(item, x[0]) for item in x[1]])  #--> PROSES MAP TERM SEBAGAI KEY, QUERY INDEKS SEBAGAI VALUE\n","\n","  # MEMPERSIAPKAN PERHITUNGAN LIKELIHOOD DATA UJI\n","  rddLikelihoodUji = rddTermUji.join(rddLikelihood).map(lambda x: ((x[1][0], x[1][1][0]), x[1][1][1])) #--> MENGGABUNGKAN \"rddTermUji\" DENGAN \"rddLikelihood\" KEMUDIAN MELAKUKAN MAPPING (QUERY INDEKS, KELAS) SEBAGAI KEY, LIKELIHOOD SEBAGAI VALUE (MEMBUANG TERM)\n","\n","  # MENGHITUNG LIKELIHOOD DATA UJI\n","  decimalUP = 1000                                                                    #--> VALUE 100 UNTUK MENGANTISIPASI PANJANG DECIMAL SAAT MENGHITUNG LIKELIHOOD                          \n","  rddLikelihoodUji = rddLikelihoodUji.reduceByKey(lambda x, y: (x * y) * decimalUP)   #--> PROSES REDUCE MENGKALIKAN SELURUH NILAI LIKELIHOOD SESUAI KEY YANG SAMA\n","  #rddLikelihoodUji = rddLikelihoodUji.reduceByKey(lambda x, y: x * y)   #--> PROSES REDUCE MENGKALIKAN SELURUH NILAI LIKELIHOOD SESUAI KEY YANG SAMA\n","  rddLikelihoodUji = rddLikelihoodUji.map(lambda x: (x[0][1], (x[0][0], x[1])))       #--> PROSES MAP KELAS SEBAGAI KEY, (QUERY INDEKS, LIKELIHOOD) SEBAGAI VALUE\n","\n","  return rddLikelihoodUji"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UVeli2QrJCwW"},"source":["##3) MENGHITUNG POSTERIOR"]},{"cell_type":"code","metadata":{"id":"umCoajMgIyuy"},"source":["def hitungPosterior(rddPrior, rddLikelihoodUji):\n","  # MENGGABUNGKAN \"rddLikelihoodUji\" DENGAN \"rddPrior\"\n","  rddPosterior = rddLikelihoodUji.join(rddPrior)\n","  rddPosterior = rddPosterior.map(lambda x: (x[1][0][0], (x[0], x[1][0][1] * x[1][1])))  #--> PROSES MAP QUERY INDEKS SEBAGAI KEY, (KELAS, HASIL PRIOR * LIKELIHOOD) SEBAGAI VALUE\n","  #rddPosterior = rddPosterior.sortByKey()  \n","\n","  print(\"  Posterior Done. . .\")\n","  return rddPosterior"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2Wmgn8dzDyJ"},"source":["# EKSEKUSI ANALISIS SENTIMEN"]},{"cell_type":"markdown","metadata":{"id":"esptYoAMz7_f"},"source":["##1) MENGAMBIL DATASET"]},{"cell_type":"code","metadata":{"id":"z9SZUCetz45Y","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":571},"executionInfo":{"status":"ok","timestamp":1609908595251,"user_tz":-420,"elapsed":32997,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}},"outputId":"3080908c-bc64-44d3-e6eb-dce3eb1f2379"},"source":["# UPLOAD DATASET YANG DIINGINKAN (.XLSX)\n","uploaded = files.upload()\n","fileName = list(uploaded.keys())[0]\n","\n","# MENGAMBIL HASIL UPLOAD DARI LIB. PANDAS, DAN DISIMPAN KE DALAM SPARK DATAFRAME  \n","df = pd.read_excel(io.BytesIO(uploaded[fileName]))     #--> \"fileName\" MERUPAKAN NAMA FILE YANG DIUPLOAD\n","dfDataset = spark.createDataFrame(df)                                           #--> MERUBAH PANDAS DATAFRAME MENJADI SPARK DATAFRAME\n","\n","# MEMBUAT RDD DATASET\n","rddDataset = dfDataset.rdd                                                      #--> MEMBERIKAN INDEKS PADA MASING-MASING DATA\n","rddDataset = rddDataset.map(lambda x: (\"D\"+str(x[0]), (x[1], x[2], x[3])))      #--> PROSES MAP (D+INDEKS) SEBAGAI KEY, (JUDUL, ISI, KELAS) SEBAGAI VALUE\n","\n","print(\"\\n======================== DATASET ========================\")\n","dfDataset.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-775e624f-f7d7-4fcb-87b6-3ccffc6332d7\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-775e624f-f7d7-4fcb-87b6-3ccffc6332d7\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving Dataset.xlsx to Dataset.xlsx\n","\n","======================== DATASET ========================\n","+----------+--------------------+--------------------+------------+\n","|Unnamed: 0|        Judul Berita|          Isi Berita|Kelas Berita|\n","+----------+--------------------+--------------------+------------+\n","|         1|Usai Gelar Doa Be...|Jakarta, CNN Indo...|     Negatif|\n","|         2|Kubu Prabowo Klai...|Liputan6.com, Jak...|     Negatif|\n","|         3|Yenny Wahid: Joko...|Putri Presiden RI...|     Negatif|\n","|         4|Quick Count LSI D...|Berbagai lembaga ...|     Positif|\n","|         5|Prabowo - Sandi u...|Ambon (ANTARA) - ...|     Positif|\n","|         6|Disebut Bagian 1 ...|Liputan6.com, Jak...|     Negatif|\n","|         7|Prabowo: Saya Tak...|Liputan6.com, Jak...|     Negatif|\n","|         8|Pemantau Pemilu A...|Jakarta, CNN Indo...|     Positif|\n","|         9|Suara Jokowi-Ma'r...|Jakarta (ANTARA) ...|     Positif|\n","|        10|Relawan Jokowi Uc...|Liputan6.com, Jak...|     Negatif|\n","|        11|Prabowo Sebut Ung...|Liputan6.com, Jak...|     Negatif|\n","|        12|Ralat Tudingan, H...|Jakarta, CNN Indo...|     Negatif|\n","|        13|Pesan Politik SBY...|Jakarta, CNN Indo...|     Positif|\n","|        14|BPN: Sandiaga Ceg...|Cawapres  tidak t...|     Positif|\n","|        15|BPN Sebut KPU Pul...|Jakarta, CNN Indo...|     Positif|\n","|        16|Dahlan Iskan Duku...|Liputan6.com, Jak...|     Negatif|\n","|        17|Jawab Pembangunan...|Liputan6.com, Jak...|     Negatif|\n","|        18|Prabowo: Industri...|Liputan6.com, Jak...|     Negatif|\n","|        19|Prabowo-Sandi ung...|Mataram (ANTARA) ...|     Positif|\n","|        20|Jokowi-Ma'ruf Ung...|Joko Widodo (Joko...|     Positif|\n","+----------+--------------------+--------------------+------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g3CCH76L0jNa"},"source":["\n","##2) MAIN PROGRAM"]},{"cell_type":"code","metadata":{"id":"bAPBgcCKJ369"},"source":["def save_RDD(rddSentimen, skenario, Supply=True):\n","  # MERUBAH MENJADI PANDAS, MEMPERBAIKI NAMA KOLOM\n","  dfSentimen = rddSentimen.map(lambda x: (x[0], x[1][0], x[1][1])).coalesce(2).toDF().toPandas()\n","  dfSentimen.columns = [\"Dokumen\", \"Kelas\", \"Value\"]\n","  dfSentimen.sort_values(by=[\"Dokumen\"], inplace=True)\n","  dfSentimen.reset_index(drop=True, inplace=True)\n","\n","  if Supply:\n","    fileName = \"Skenario-1.\"+str(skenario+1)+\".csv\"\n","  else:\n","    fileName = \"Skenario-2.\"+str(skenario+1)+\".csv\"\n","    \n","  dfFile = dfSentimen\n","  dfFile.to_csv(fileName, index=False)\n","  files.download(fileName)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUgGRazYFjrs"},"source":["def printRDD(rddSpark, All=True):\n","  if All:\n","    print(\"  \" + str(type(rddSpark)))\n","    print(\"  \" + str(rddSpark.count()))\n","  for x in rddSpark.collect():\n","    print(\"  \" + str(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RsIa8ijPE8jP"},"source":["###2.1) SUPPLY TRAINING TEST"]},{"cell_type":"code","metadata":{"id":"6Vs_dGXc0ikW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609908634528,"user_tz":-420,"elapsed":22425,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}},"outputId":"6faa7bea-b9b8-4909-fca7-88492a51b058"},"source":["# MELAKUKAN PERHITUNGAN PERSENTASE ANTARA DATA LATIH & DATA UJI\n","persentase = [60, 70, 80, 90]\n","#persentase = [80]\n","totalDataset = dfDataset.count()                   #--> \"totalDataset\" MENGAMBIL TOTAL DATASET\n","\n","for skenario, value in enumerate(persentase):\n","  start_time = time.time()                         #--> \"startTime\" MENGAMBIL CURRENT TIME\n","  drainData = round((totalDataset*value)/100)      #--> \"drainData\" SEBAGAI VARIABEL BERNILAI JUMLAH DATA LATIH YANG DIGUNAKAN\n","\n","  print(\"============================================= SKENARIO - \"+ str(skenario+1) + \" =============================================\")\n","  print(\"Jumlah Data Latih = \" + str(drainData))\n","  print(\"Jumlah Data Uji   = \" + str(totalDataset-drainData))\n","  \n","  print(\"\\n> Memisahkan Data Latih & Data Uji\")\n","  # MEMBUAT DATAFRAME BARU ANTARA POSITIF & NEGATIF\n","  dfPositif = dfDataset.filter(col(\"Kelas Berita\") == \"Positif\")\n","  dfNegatif = dfDataset.filter(col(\"Kelas Berita\") == \"Negatif\")\n","\n","  dfDataLatih = get_DF_DataLatih(dfNegatif, dfPositif)\n","  rddDataLatih = get_RDD_DataLatih(dfDataLatih)\n","  dfDataUji = get_DF_DataUji(dfDataset, dfDataLatih)\n","  rddDataUji = get_RDD_DataUji(dfDataUji)\n","  print(\"  Done. . .\")\n","\n","  print(\"\\n> Melakukan Text Preprocessing\")\n","  # MENYIAPKAN RDD UNTUK PROSES TEXT PREPROCESSING\n","  rddTextPrep = rddDataset.map(lambda x: (x[0], str(x[1][0]+\" \"+x[1][1])))      #--> MENGAMBIL SELURUH DATA UNTUK DILAKUKAN TEKS PREPROCESSING\n","  \n","  # MENGGABUNGKAN RDD DATA LATIH DAN DATA UJI UNTUK DILAKUKAN TEXT PREPROCESSING\n","  #rddTextPrep = rddDataLatih.union(rddDataUji)\n","  #rddTextPrep = rddTextPrep.zipWithIndex()       #--> MEN-ZIP RDD UNTUK DIBERIKAN INDEKS PADA SETIAP DATA\n","\n","  rddToken = Tokenizing(rddTextPrep)             #--> PROSES TOKENIZING\n","  rddFilter = Filtering(rddToken)                #--> PROSES FILTERING\n","  rddStem = Stemming(rddFilter)                  #--> PROSES STEMMING\n","  print(\"  Done. . .\")\n","\n","  print(\"\\n> Melakukan Term Weighting\")\n","  # MENGAMBIL KELAS PADA DATA LATIH\n","  dfKelasDataLatih = dfDataLatih.select(col(\"Kelas Berita\"), col(\"Unnamed: 0\")) #--> KELAS DATA LATIH DALAM BENTUK SPARK DATAFRAME\n","  rddKelasDataLatih = dfKelasDataLatih.rdd                                      #--> KELAS DATA LATIH DALAM BENTUK SPARK RDD\n","  \n","  # MEMBUAT KEY BERUPA INDEKS DOKUMEN\n","  rddKelasDataLatih = rddKelasDataLatih.map(lambda x: (\"D\"+str(x[1]), x[0]))    #--> PROSES MAP (D+INDEKS) SEBAGAI KEY, KELAS SEBAGAI VALUE\n","\n","  # MEMBUAT TERM DATA LATIH\n","  rddTerm = rddKelasDataLatih.join(rddStem)                                     #--> MENGAMBIL DARI HASIL STEMMING DENGAN PROSES JOIN DARI \"rddKelasDataLatih\"\n","  rddTerm = rddTerm.map(lambda x: ((x[0], x[1][0]), x[1][1]))                   #--> PROSES MAP (DOK. INDEKS, KELAS) SEBAGAI KEY, STEM SEBAGAI VALUE\n","\n","  countDoc = drainData                                                          #--> MENGAMBIL TOTAL DOKUMEN DATA LATIH\n","  #countDoc = rddTerm.count()                                                   \n","\n","  rddTF = hitungTF(rddTerm)                                                     #--> HITUNG LOG-TF\n","  rddDF = hitungDF(rddTerm)                                                     #--> HITUNG DF\n","  rddIDF = hitungIDF(rddDF, countDoc)                                           #--> HITUNG IDF\n","  rddTFIDF = hitungTFIDF(rddTF, rddIDF, rddKelasDataLatih)                      #--> HITUNG TF-IDF\n","  print(\"  Done. . .\")\n","\n","  print(\"\\n> Melakukan Analisis Sentimen\")\n","  # MENGAMBIL TOTAL TERM DATA LATIH DISIMPAN KEDALAM RDD\n","  countTerm = rddIDF.map(lambda x: (\"[KEY_COUNT_TERM]\",1)).reduceByKey(lambda x, y: x + y)  #--> PROSES MAP [KEY_COUNT_TERM] SEBAGAI KEY, JUMLAH KATA SEBAGAI VALUE\n","\n","  rddPrior = hitungPrior(rddKelasDataLatih, countDoc)                                       #--> HITUNG PRIOR\n","  rddLikelihood = hitungLikelihood(rddIDF, rddTFIDF, rddStem, rddDataUji, countTerm)        #--> HITUNG LIKELIHOOD\n","  rddPosterior = hitungPosterior(rddPrior, rddLikelihood)                                   #--> HITUNG POSTERIOR\n","\n","  #printRDD(rddPosterior)\n","  print(\"  Done. . .\")\n","\n","  print(\"\\n> Hasil Klasifikasi Masing-Masing Data Uji\")\n","  # MENGAMBIL NILAI TERTINGGI (PELUANG TERTINGGI)\n","  rddSentimen = rddPosterior.reduceByKey(lambda x, y: (x[0], x[1]) if x[1] > y[1] else (y[0], y[1])) #--> PROSES REDUCE UNTUK MENCARI NILAI TERTINGGI DARI MASING-MASING KEY\n","  #printRDD(rddSentimen, False)\n","  print(\"  Done. . .\")\n","  \n","  print(\"\\n--- %.2f seconds ---\" % (time.time() - start_time))\n","  #save_RDD(rddSentimen, skenario)\n","  #break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["============================================= SKENARIO - 1 =============================================\n","Jumlah Data Latih = 60\n","Jumlah Data Uji   = 40\n","\n","> Memisahkan Data Latih & Data Uji\n","  Done. . .\n","\n","> Melakukan Text Preprocessing\n","  Tokenizing Done. . .\n","  Filtering Done. . .\n","  Stemming Done. . .\n","  Done. . .\n","\n","> Melakukan Term Weighting\n","  TF Done. . .\n","  DF Done. . .\n","  IDF Done. . .\n","  TFIDF Done. . .\n","  Done. . .\n","\n","> Melakukan Analisis Sentimen\n","  Prior Done. . .\n","  Likelihood Done. . .\n","  Posterior Done. . .\n","  Done. . .\n","\n","> Hasil Klasifikasi Masing-Masing Data Uji\n","  Done. . .\n","\n","--- 7.84 seconds ---\n","============================================= SKENARIO - 2 =============================================\n","Jumlah Data Latih = 70\n","Jumlah Data Uji   = 30\n","\n","> Memisahkan Data Latih & Data Uji\n","  Done. . .\n","\n","> Melakukan Text Preprocessing\n","  Tokenizing Done. . .\n","  Filtering Done. . .\n","  Stemming Done. . .\n","  Done. . .\n","\n","> Melakukan Term Weighting\n","  TF Done. . .\n","  DF Done. . .\n","  IDF Done. . .\n","  TFIDF Done. . .\n","  Done. . .\n","\n","> Melakukan Analisis Sentimen\n","  Prior Done. . .\n","  Likelihood Done. . .\n","  Posterior Done. . .\n","  Done. . .\n","\n","> Hasil Klasifikasi Masing-Masing Data Uji\n","  Done. . .\n","\n","--- 4.83 seconds ---\n","============================================= SKENARIO - 3 =============================================\n","Jumlah Data Latih = 80\n","Jumlah Data Uji   = 20\n","\n","> Memisahkan Data Latih & Data Uji\n","  Done. . .\n","\n","> Melakukan Text Preprocessing\n","  Tokenizing Done. . .\n","  Filtering Done. . .\n","  Stemming Done. . .\n","  Done. . .\n","\n","> Melakukan Term Weighting\n","  TF Done. . .\n","  DF Done. . .\n","  IDF Done. . .\n","  TFIDF Done. . .\n","  Done. . .\n","\n","> Melakukan Analisis Sentimen\n","  Prior Done. . .\n","  Likelihood Done. . .\n","  Posterior Done. . .\n","  Done. . .\n","\n","> Hasil Klasifikasi Masing-Masing Data Uji\n","  Done. . .\n","\n","--- 4.25 seconds ---\n","============================================= SKENARIO - 4 =============================================\n","Jumlah Data Latih = 90\n","Jumlah Data Uji   = 10\n","\n","> Memisahkan Data Latih & Data Uji\n","  Done. . .\n","\n","> Melakukan Text Preprocessing\n","  Tokenizing Done. . .\n","  Filtering Done. . .\n","  Stemming Done. . .\n","  Done. . .\n","\n","> Melakukan Term Weighting\n","  TF Done. . .\n","  DF Done. . .\n","  IDF Done. . .\n","  TFIDF Done. . .\n","  Done. . .\n","\n","> Melakukan Analisis Sentimen\n","  Prior Done. . .\n","  Likelihood Done. . .\n","  Posterior Done. . .\n","  Done. . .\n","\n","> Hasil Klasifikasi Masing-Masing Data Uji\n","  Done. . .\n","\n","--- 4.12 seconds ---\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iySExUOBVo2d","colab":{"base_uri":"https://localhost:8080/","height":123},"executionInfo":{"status":"ok","timestamp":1595570070670,"user_tz":-420,"elapsed":77267,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}},"outputId":"f0d666b4-e1db-4f71-b1bb-669c4d3f95d6"},"source":["printRDD(rddPosterior)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  <class 'pyspark.rdd.PipelinedRDD'>\n","  4\n","  ('Q8', ('Negatif', 1.0174248159797753e-173))\n","  ('Q7', ('Negatif', 8.112083706356538e-107))\n","  ('Q7', ('Positif', 1.0173410952115083e-112))\n","  ('Q8', ('Positif', 9.851889959821554e-169))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":16},"id":"HN-Y-goXCJ1W","executionInfo":{"status":"ok","timestamp":1609909166194,"user_tz":-420,"elapsed":3615,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}},"outputId":"685134e2-de4d-478e-a549-acb4fdb295ab"},"source":["Supply = True\r\n","  # MERUBAH MENJADI PANDAS, MEMPERBAIKI NAMA KOLOM\r\n","dfSentimen = rddSentimen.map(lambda x: (x[0], x[1][0], x[1][1])).coalesce(2).toDF().toPandas()\r\n","dfSentimen.columns = [\"Dokumen\", \"Kelas\", \"Value\"]\r\n","dfSentimen.sort_values(by=[\"Dokumen\"], inplace=True)\r\n","dfSentimen.reset_index(drop=True, inplace=True)\r\n","\r\n","if Supply:\r\n","  fileName = \"Skenario-1.\"+str(skenario+1)+\".csv\"\r\n","else:\r\n","  fileName = \"Skenario-2.\"+str(skenario+1)+\".csv\"\r\n","    \r\n","dfFile = dfSentimen\r\n","dfFile.to_csv(fileName, index=False)\r\n","files.download(fileName)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_8ac6f483-3d9d-47d2-aeee-7706eb6963ab\", \"Skenario-1.4.csv\", 364)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"mGnIrLetFDG3"},"source":["###2.2) K FOLD CROSS VALIDATION"]},{"cell_type":"code","metadata":{"id":"V5xWf7CwI-zR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609328783692,"user_tz":-420,"elapsed":11368,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}},"outputId":"d79a41b9-d03f-49c2-b0cf-66facf6ce36c"},"source":["# MENENTUKAN JUMLAH K UNTUK PROSES FOLD\n","k = 2\n","divideData = (dfDataset.count()/2)/k\n","listDF_Negatif = []\n","listDF_Positif = []\n","\n","dfNegatif = dfDataset.filter(col(\"Kelas Berita\") == \"Negatif\").toPandas()\n","dfPositif = dfDataset.filter(col(\"Kelas Berita\") == \"Positif\").toPandas()\n","\n","for i in range(k):\n","  listDF_Negatif.append(dfNegatif.iloc[int(divideData*i):int(divideData*(i+1))])\n","  listDF_Positif.append(dfPositif.iloc[int(divideData*i):int(divideData*(i+1))])\n","\n","for kFold in range(k):\n","  start_time = time.time()                                       #--> \"startTime\" MENGAMBIL CURRENT TIME\n","  drainData = dfDataset.count() - round(dfDataset.count()/k)     #--> \"drainData\" SEBAGAI VARIABEL BERNILAI JUMLAH DATA LATIH YANG DIGUNAKAN\n","\n","  print(\"============================================= SKENARIO - \"+ str(kFold+1) + \" =============================================\")\n","  print(\"Jumlah Data Latih = \" + str(drainData))\n","  print(\"Jumlah Data Uji   = \" + str(dfDataset.count()-drainData))\n","  \n","  print(\"\\n> Memisahkan Data Latih & Data Uji\")\n","  # MEMBUAT DATAFRAME BARU ANTARA POSITIF & NEGATIF\n","  dfDataUji = get_DF_DataUji_KFOLD(listDF_Negatif[kFold], listDF_Positif[kFold])\n","  rddDataUji = get_RDD_DataUji(dfDataUji)\n","  dfDataLatih = get_DF_DataLatih_KFOLD(dfDataset, dfDataUji)\n","  rddDataLatih = get_RDD_DataLatih(dfDataLatih)\n","\n","  #dfDataUji = get_DF_DataUji(dfDataset, dfDataLatih)\n","  #rddDataUji = get_RDD_DataUji(dfDataUji)\n","  print(\"  Done. . .\")\n","\n","  print(\"\\n> Melakukan Text Preprocessing\")\n","  # MENYIAPKAN RDD UNTUK PROSES TEXT PREPROCESSING\n","  rddTextPrep = rddDataset.map(lambda x: (x[0], str(x[1][0]+\" \"+x[1][1])))      #--> MENGAMBIL SELURUH DATA UNTUK DILAKUKAN TEKS PREPROCESSING\n","  \n","  # MENGGABUNGKAN RDD DATA LATIH DAN DATA UJI UNTUK DILAKUKAN TEXT PREPROCESSING\n","  #rddTextPrep = rddDataLatih.union(rddDataUji)\n","  #rddTextPrep = rddTextPrep.zipWithIndex()       #--> MEN-ZIP RDD UNTUK DIBERIKAN INDEKS PADA SETIAP DATA\n","\n","  rddToken = Tokenizing(rddTextPrep)             #--> PROSES TOKENIZING\n","  rddFilter = Filtering(rddToken)                #--> PROSES FILTERING\n","  rddStem = Stemming(rddFilter)                  #--> PROSES STEMMING\n","  print(\"  Done. . .\")\n","\n","  print(\"\\n> Melakukan Term Weighting\")\n","  # MENGAMBIL KELAS PADA DATA LATIH\n","  dfKelasDataLatih = dfDataLatih.select(col(\"Kelas Berita\"), col(\"Unnamed: 0\"))   #--> KELAS DATA LATIH DALAM BENTUK SPARK DATAFRAME\n","  rddKelasDataLatih = dfKelasDataLatih.rdd                                        #--> KELAS DATA LATIH DALAM BENTUK SPARK RDD\n","  \n","  # MEMBUAT KEY BERUPA INDEKS DOKUMEN\n","  rddKelasDataLatih = rddKelasDataLatih.map(lambda x: (\"D\"+str(x[1]), x[0]))      #--> PROSES MAP (D+INDEKS) SEBAGAI KEY, KELAS SEBAGAI VALUE\n","\n","  # MEMBUAT TERM DATA LATIH\n","  rddTerm = rddKelasDataLatih.join(rddStem)                                     #--> MENGAMBIL DARI HASIL STEMMING DENGAN PROSES JOIN DARI \"rddKelasDataLatih\"\n","  rddTerm = rddTerm.map(lambda x: ((x[0], x[1][0]), x[1][1]))                   #--> PROSES MAP (DOK. INDEKS, KELAS) SEBAGAI KEY, STEM SEBAGAI VALUE\n","\n","  countDoc = drainData                                                          #--> MENGAMBIL TOTAL DOKUMEN DATA LATIH\n","  #countDoc = rddTerm.count()                                                   \n","\n","  rddTF = hitungTF(rddTerm)                                                     #--> HITUNG LOG-TF\n","  rddDF = hitungDF(rddTerm)                                                     #--> HITUNG DF\n","  rddIDF = hitungIDF(rddDF, countDoc)                                           #--> HITUNG IDF\n","  rddTFIDF = hitungTFIDF(rddTF, rddIDF, rddKelasDataLatih)                      #--> HITUNG TF-IDF\n","  print(\"  Done. . .\")\n","\n","  print(\"\\n> Melakukan Analisis Sentimen\")\n","  # MENGAMBIL TOTAL TERM DATA LATIH DISIMPAN KEDALAM RDD\n","  countTerm = rddIDF.map(lambda x: (\"[KEY_COUNT_TERM]\",1)).reduceByKey(lambda x, y: x + y)  #--> PROSES MAP [KEY_COUNT_TERM] SEBAGAI KEY, JUMLAH KATA SEBAGAI VALUE\n","\n","  rddPrior = hitungPrior(rddKelasDataLatih, countDoc)                                       #--> HITUNG PRIOR\n","  rddLikelihood = hitungLikelihood(rddIDF, rddTFIDF, rddStem, rddDataUji, countTerm)        #--> HITUNG LIKELIHOOD\n","  rddPosterior = hitungPosterior(rddPrior, rddLikelihood)                                   #--> HITUNG POSTERIOR\n","\n","  #printRDD(rddPosterior)\n","  print(\"  Done. . .\")\n","\n","  print(\"\\n> Hasil Klasifikasi Masing-Masing Data Uji\")\n","  # MENGAMBIL NILAI TERTINGGI (PELUANG TERTINGGI)\n","  rddSentimen = rddPosterior.reduceByKey(lambda x, y: (x[0], x[1]) if x[1] > y[1] else (y[0], y[1])) #--> PROSES REDUCE UNTUK MENCARI NILAI TERTINGGI DARI MASING-MASING KEY\n","  #printRDD(rddSentimen, False)\n","  print(\"  Done. . .\")\n","  \n","  print(\"\\n--- %.2f seconds ---\" % (time.time() - start_time))\n","  #save_RDD(rddSentimen, kFold, False)\n","  #break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["============================================= SKENARIO - 1 =============================================\n","Jumlah Data Latih = 50\n","Jumlah Data Uji   = 50\n","\n","> Memisahkan Data Latih & Data Uji\n","  Done. . .\n","\n","> Melakukan Text Preprocessing\n","  Tokenizing Done. . .\n","  Filtering Done. . .\n","  Stemming Done. . .\n","  Done. . .\n","\n","> Melakukan Term Weighting\n","  TF Done. . .\n","  DF Done. . .\n","  IDF Done. . .\n","  TFIDF Done. . .\n","  Done. . .\n","\n","> Melakukan Analisis Sentimen\n","  Prior Done. . .\n","  Likelihood Done. . .\n","  Posterior Done. . .\n","  Done. . .\n","\n","> Hasil Klasifikasi Masing-Masing Data Uji\n","  Done. . .\n","\n","--- 5.18 seconds ---\n","============================================= SKENARIO - 2 =============================================\n","Jumlah Data Latih = 50\n","Jumlah Data Uji   = 50\n","\n","> Memisahkan Data Latih & Data Uji\n","  Done. . .\n","\n","> Melakukan Text Preprocessing\n","  Tokenizing Done. . .\n","  Filtering Done. . .\n","  Stemming Done. . .\n","  Done. . .\n","\n","> Melakukan Term Weighting\n","  TF Done. . .\n","  DF Done. . .\n","  IDF Done. . .\n","  TFIDF Done. . .\n","  Done. . .\n","\n","> Melakukan Analisis Sentimen\n","  Prior Done. . .\n","  Likelihood Done. . .\n","  Posterior Done. . .\n","  Done. . .\n","\n","> Hasil Klasifikasi Masing-Masing Data Uji\n","  Done. . .\n","\n","--- 4.72 seconds ---\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q8_V2U0FYjqM","colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"status":"ok","timestamp":1596445132682,"user_tz":-420,"elapsed":380717,"user":{"displayName":"Reza Fauzi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6tNF0SZY3D6iGHxj04iyb3Y86Tl5GPHxKTj21Vw=s64","userId":"09160483363598746448"}},"outputId":"53aeb05a-e405-46de-f806-3009f386fc96"},"source":["printRDD(rddSentimen)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  <class 'pyspark.rdd.PipelinedRDD'>\n","  20\n","  ('Q13', ('Positif', 1.6806187227324713e-21))\n","  ('Q9', ('Positif', 956992590437.9546))\n","  ('Q4', ('Positif', 2059079681.2463188))\n","  ('Q8', ('Positif', 3.502637542626532e-25))\n","  ('Q5', ('Positif', 3132961876285533.5))\n","  ('Q17', ('Negatif', 1.836874065913085e-12))\n","  ('Q1', ('Positif', 2.6001650660938725e-10))\n","  ('Q3', ('Positif', 0.015042499942247442))\n","  ('Q10', ('Negatif', 0.7615935585376525))\n","  ('Q20', ('Positif', 159464.70440061396))\n","  ('Q14', ('Positif', 7.601443234325275e-11))\n","  ('Q6', ('Negatif', 1.5545720053244828e-09))\n","  ('Q7', ('Negatif', 1.0678737862918983e-06))\n","  ('Q16', ('Negatif', 3.383931280765734e-08))\n","  ('Q19', ('Positif', 545259057.8063266))\n","  ('Q11', ('Positif', 209803220830.41476))\n","  ('Q12', ('Positif', 3.838361458808537e-11))\n","  ('Q21', ('Negatif', 1.7668210477363984e-12))\n","  ('Q2', ('Positif', 1.4872640895076548e-05))\n","  ('Q15', ('Positif', 4.343689536069084e-22))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"quo3tpSB3ZqC"},"source":["#DUMP"]},{"cell_type":"code","metadata":{"id":"51Xkm3EM3c5R"},"source":["def delokSek(rddSpark):\n","  for x in rddSpark.collect():\n","    #print(type(x))\n","    print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vdfr2XzNFMf"},"source":["pandasDataset = dfDataset.toPandas()\n","\n","display(pandasDataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfSaT46dPEXo"},"source":["k = 5\n","divideData = (dfDataset.count()/2)/5\n","listDF_Negatif = []\n","listDF_Positif = []\n","\n","dfNegatif = dfDataset.filter(col(\"Kelas Berita\") == \"Negatif\").toPandas()\n","dfPositif = dfDataset.filter(col(\"Kelas Berita\") == \"Positif\").toPandas()\n","\n","for i in range(k):\n","  listDF_Negatif.append(dfNegatif.iloc[int(divideData*i):int(divideData*(i+1))])\n","  listDF_Positif.append(dfPositif.iloc[int(divideData*i):int(divideData*(i+1))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sq8ji32zSse5"},"source":["dfLatihNegatif = spark.createDataFrame(dfNegatif)\n","dfLatihPositif = spark.createDataFrame(dfPositif)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Zs7OaW3NFMn"},"source":["dataset.filter(col(\"Kelas\") == \"Negatif\").show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"464h6VmINFMt"},"source":["w = dfDataset.count()\n","print(w)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAwK02MgNFM2"},"source":["# MENCOBA EXPLODE\n","rddTF = rddTerm.flatMap(lambda x: [((x[0][0], item), 1) for item in x[1]])\n","dfTF = rddTerm.toDF()\n","\n","df_exploded = dfTF.withColumn('_2', explode('_2'))\n","\n","dfTF.show()\n","df_exploded.show()\n","\n","rddTF = df_exploded.rdd\n","rddTF = rddTF.map(lambda x: ((x[0][0], x[1]), 1))\n","printRDD(rddTF, False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEJD2DNhNFM8"},"source":["start_time = time.time()\n","\n","#countTerm = rddIDF                                              #--> MENGAMBIL TOTAL TERM DATA LATIH\n","#countTerm = countTerm.repartition(2).count()                                           #--> MENGAMBIL TOTAL TERM DATA LATIH\n","\n","countTerm = rddIDF.countApprox(10,1.0)\n","print(countTerm)\n","\n","print(\"--- %.2f seconds ---\" % (time.time() - start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6ZP-sTzNFNC"},"source":["\n","  rddStemDataLatih = rddTerm.map(lambda x: (x[0][0], x[1])) #--> MENGAMBIL \"rddTerm\" DENGAN PROSES MAP DOC. INDEKS SEBAGAI KEY, TERM SEBAGAI VALUE\n","\n","  # MENGAMBIL DATA UJI DENGAN LEFT ANTI JOIN\n","  dfStemDataUji = rddStem.toDF().join(rddStemDataLatih.toDF(), [\"_1\", \"_2\"], \"leftanti\")  #--> MENGGABUNGKAN \"rddStem\" DENGAN \"rddStemDataLatih\" KEMUDIAN MENGABIL DATA YANG TIDAK SAMA\n","  rddStemDataUji = dfStemDataUji.select(col(\"_2\")).rdd.flatMap(lambda x: list(x))         #--> MENGAMBIL KOLOM \"_2\" DARI DATAFRAME, KARENA BERISI TERM DATA UJI\n","\n","\n","  # PROSES MAPPING PADA DATA UJI\n","  rddStemDataUji = rddStemDataUji.zipWithIndex()                          #--> MEMBERIKAN INDEKS PADA DATA UJI\n","  rddStemDataUji = rddStemDataUji.map(lambda x: (\"Q\"+str(x[1]+1), x[0]))  #--> PROSES MAP (Q+INDEKS) SEBAGAI KEY, TERM LIST SEBAGAI VALUE\n","  #printRDD(rddStemDataUji, False) #<--\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKbDZBKKNFNK"},"source":["def hitungLikelihood(countTerm, rddTFIDF, rddTerm, rddStem, rddDataUji):       \n","  # MENGAMBIL NILAI IDF UNTUK MEMISAHKAN TERM MASING-MASING KELAS\n","  rddLikelihood = rddIDF.map(lambda x: (x[0], [\"Negatif\", \"Positif\"]))              #--> PROSES MAP TERM SEBAGAI KEY, (NEGATIF, POISITF) SEBAGAI VALUE DARI \"rddIDF\"\n","  rddLikelihood = rddLikelihood.flatMap(lambda x: [(item, x[0]) for item in x[1]])  #--> PROSES MAP MEMISAHKAN SETIAP KELAS, KELAS SEBAGAI KEY, TERM SEBAGAI VALUE\n","\n","  # MENGHITUNG/COUNT SELURUH TW DARI MASING-MASING KELAS\n","  rddTotalTermClass = rddTFIDF.map(lambda x: (x[0][2], x[1]))             #--> PROSES MAP KELAS SEBAGAI KEY, TF-IDF SEBAGAI VALUE\n","  rddTotalTermClass = rddTotalTermClass.reduceByKey(lambda x, y: x + y)   #--> PROSES REDUCE MENJUMLAHKAN VALUE SESUAI KEY YANG SAMA\n","\n","  # MENGGABUNGKAN HASIL COUNT SELURUH TW KE DALAM \"rddLikelihood\"\n","  rddLikelihood = rddLikelihood.join(rddTotalTermClass).map(lambda x: ((x[1][0], x[0]), x[1][1])) #--> MENGGABUNGKAN DENGAN \"rddTotalTermClass\" KEMUDIAN MELAKUKAN MAPPING (TERM, KELAS) SEBAGAI KEY, COUNT TW SEBAGAI VALUE\n","  \n","  # MENGHIUTNG/COUNT TERM TW DARI MASING-MASING KELAS\n","  rddTotalTermEachClass = rddIDF.map(lambda x: (x[0], [\"Negatif\", \"Positif\"]))                            #--> PROSES MAP TERM SEBAGAI KEY, (NEGATIF, POISITF) SEBAGAI VALUE DARI \"rddIDF\"\n","  rddTotalTermEachClass = rddTotalTermEachClass.flatMap(lambda x: [((x[0], item), 0) for item in x[1]])   #--> PROSES MAP MEMISAHKAN SETIAP KELAS, (TERM, KELAS) SEBAGAI KEY, 0 SEBAGAI VALUE\n","\n","  rddT_TFIDF_Term = rddTFIDF.map(lambda x: ((x[0][1], x[0][2]), x[1]))                                    #--> PROSES MAP (TERM, KELAS) SEBAGAI KEY, TF-IDF SEBAGAI VALUE DARI \"rddTFIDF\"\n","  rddT_TFIDF_Term = rddT_TFIDF_Term.reduceByKey(lambda x, y: x + y)                                       #--> PROSES REDUCE MENJUMLAHKAN VALUE SESUAI KEY YANG SAMA\n","\n","  rddTotalTermEachClass = rddTotalTermEachClass.leftOuterJoin(rddT_TFIDF_Term)                              #--> MENGGABUNGKAN RDD UNTUK MENGAMBIL COUNT TERM TW DARI MASING-MASING KELAS\n","  rddTotalTermEachClass = rddTotalTermEachClass.mapValues(lambda x: x[0] + x[1] if not x[1] is None else 0) #--> MENGHITUNG COUNT TERM TW, APABILA VALUE \"None\" BERARTI COUNT TERM BERNILAI 0\n","\n","  # MENGGABUNGKAN HASIL COUNT TERM TW KE DALAM \"rddLikelihood\"\n","  rddLikelihood = rddLikelihood.join(rddTotalTermEachClass).map(lambda x: (x[0][0], x[0][1], x[1][0], x[1][1]))    #--> MENGGABUNGKAN DENGAN \"rddTotalTermEachClass\" KEMUDIAN MELAKUKAN MAPPING (TERM, KELAS, COUNT TW, COUNT TERM TW) SEBAGAI VALUE\n","  ###rddLikelihood = rddLikelihood.reduceByKey(lambda k, v: k + v)                                                 #--> PROSES REDUCE MENJUMLAHKAN VALUE SESUAI KEY YANG SAMA\n","\n","  # MELAKUKAN MAPPING \"rddLikelihood\" UNTUK MENGAMBIL NILAI TOTAL TERM\n","  rddLikelihood = rddLikelihood.map(lambda x: (\"[KEY_COUNT_TERM]\", x))    #--> PROSES MAP [KEY_COUNT_TERM] SEBAGAI KEY, (TERM, KELAS, COUNT TW, COUNT TERM TW) SEBAGAI VALUE\n","  rddLikelihood = countTerm.join(rddLikelihood)                           #--> MENGGABUNGKAN RDD UNTUK MENGAMBIL TOTAL TERM DATA LATIH DARI \"countTerm\"\n","\n","  # MENGHITUNG MULTINOMIAL NAIVE BAYES (LIKELIHOOD)\n","  rddLikelihood = rddLikelihood.map(lambda x: (x[1][1][0], (x[1][1][1], (x[1][1][3] + 1)/(x[1][1][2] + x[1][0])))) \n","\n","  rddLikelihoodUji = hitungLikelihoodUji(rddTerm, rddStem, rddLikelihood, rddDataUji)\n","\n","  print(\"  Likelihood Done. . .\")\n","  return rddLikelihoodUji"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dh9URevPNFNQ"},"source":["def get_RDD_DataLatih(dfDataLatih):  \n","  df = dfDataLatih.select(concat(col(\"Judul Berita\"), lit(\" \"), col(\"Isi Berita\"))) #--> MENGGABUNGKAN JUDUL BERITA DENGAN ISI BERITA\n","  rddDataLatih = df.rdd.flatMap(lambda x: list(x))                                  #--> MENYIMPAN DATA LATIH KE DALAM SPARK RDD\n","\n","  return rddDataLatih"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CmTI0tz6NFNY"},"source":["def get_RDD_DataUji(dfDataUji):  \n","  df = dfDataUji.select(concat(col(\"Judul Berita\"), lit(\" \"), col(\"Isi Berita\"))) #--> MENGGABUNGKAN JUDUL BERITA DENGAN ISI BERITA\n","  rddDataUji = df.rdd.flatMap(lambda x: list(x))                                  #--> MENYIMPAN DATA LATIH KE DALAM SPARK RDD\n","\n","  return rddDataUji"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHCmusFuNFNf"},"source":["def hitungLikelihoodUji(rddTerm, rddStem, rddLikelihood, rddDataUji):\n","  # MEMPERSIAPKAN TERM DATA UJI\n","  rddStemDataUji = rddStem.join(rddDataUji)  #--> MENGAMBIL DATA UJI DENGAN MENGGABUNGKAN SESUAI KEY YANG ADA PADA \"rddStemn\" & \"rddStemDataUji\"\n","\n","  #PROSES MAPPING UNTUK MERUBAH KEY DAN VALUE \"rddStemDataUji\"\n","  rddStemDataUji = rddStemDataUji.map(lambda x: (\"Q\"+str(int(x[0][1:])), x[1][0]))  #--> QUERY INDEKS SEBAGAI KEY, TERM LIST SEBAGAI VALUE\n","\n","  # MENJADIKAN TERM SEBAGAI DATA RDD\n","  rddTermUji = rddStemDataUji.flatMap(lambda x: [(item, x[0]) for item in x[1]])  #--> PROSES MAP TERM SEBAGAI KEY, QUERY INDEKS SEBAGAI VALUE\n","\n","  # MEMPERSIAPKAN PERHITUNGAN LIKELIHOOD DATA UJI\n","  rddLikelihoodUji = rddTermUji.join(rddLikelihood).mapValues(lambda x: (x[0], x[1][0], x[1][1])) #--> MENGGABUNGKAN \"rddTermUji\" DENGAN \"rddLikelihood\" KEMUDIAN MELAKUKAN MAPPING (QUERY INDEKS, KELAS, LIKELIHOOD) SEBAGAI VALUE\n","\n","  # MENGHITUNG LIKELIHOOD DATA UJI\n","  rddLikelihoodUji = rddLikelihoodUji.map(lambda x: ((x[1][0], x[1][1]), x[1][2]))                              #--> MEMBUANG TERM, PROSES MAP (QUERY INDEKS, KELAS) SEBAGAI KEY, LIKELIHOOD SEBAGAI VALUE\n","  rddLikelihoodUji = rddLikelihoodUji.reduceByKey(lambda x, y: x * y).map(lambda x: (x[0][1], (x[0][0], x[1]))) #--> PROSES REDUCE MENGKALIKAN SELURUH NILAI LIKELIHOOD, KEMUDIAN MELAKUKAN MAPPING KELAS SEBAGAI KEY, (QUERY INDEKS, LIKELIHOOD) SEBAGAI VALUE\n","\n","  return rddLikelihoodUji"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kZl6p8UNNFNn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tRgxsfsNFNu"},"source":[""],"execution_count":null,"outputs":[]}]}